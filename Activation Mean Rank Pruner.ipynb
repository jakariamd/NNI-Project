{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559a9102",
   "metadata": {},
   "source": [
    "# Load Pretrained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63519f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from nni.compression.pytorch.speedup import ModelSpeedup\n",
    "from nni.compression.pytorch.utils import count_flops_params\n",
    "import time\n",
    "\n",
    "from mnist_model import Net, train, test, device, optimizer_scheduler_generator, trainer\n",
    "\n",
    "# Load pretrained model\n",
    "model = torch.load(\"mnist_cnn.pt\")\n",
    "model.eval()\n",
    "\n",
    "# show the model stbructure, note that pruner will wrap the model layer.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4435816",
   "metadata": {},
   "source": [
    "### Performance and statistics of pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65e18e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0267, Accuracy: 9919/10000 (99.19%)\n",
      "\n",
      "+-------+-------+--------+----------------+-----------------+-----------------+----------+---------+\n",
      "| Index | Name  |  Type  |  Weight Shape  |    Input Size   |   Output Size   |  FLOPs   | #Params |\n",
      "+-------+-------+--------+----------------+-----------------+-----------------+----------+---------+\n",
      "|   0   | conv1 | Conv2d | (32, 1, 3, 3)  |  (3, 1, 28, 28) | (3, 32, 26, 26) |  194688  |   320   |\n",
      "|   1   | conv2 | Conv2d | (64, 32, 3, 3) | (3, 32, 26, 26) | (3, 64, 24, 24) | 10616832 |  18496  |\n",
      "|   2   | fc1   | Linear |  (128, 9216)   |    (3, 9216)    |     (3, 128)    | 1179648  | 1179776 |\n",
      "|   3   | fc2   | Linear |   (10, 128)    |     (3, 128)    |     (3, 10)     |   1280   |   1290  |\n",
      "+-------+-------+--------+----------------+-----------------+-----------------+----------+---------+\n",
      "FLOPs total: 11992448\n",
      "#Params total: 1199882\n",
      "Pretrained model FLOPs 11.99 M, #Params: 1.20M, Accuracy:  99.19%, Test-time:  1.7579s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pre_best_acc = test(model, device)\n",
    "pre_test_time = time.time() - start\n",
    "\n",
    "pre_flops, pre_params, _ = count_flops_params(model, torch.randn([3, 1, 28, 28]).to(device))\n",
    "print(f'Pretrained model FLOPs {pre_flops/1e6:.2f} M, #Params: {pre_params/1e6:.2f}M, Accuracy: {pre_best_acc: .2f}%, Test-time: {pre_test_time: .4f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba20bd",
   "metadata": {},
   "source": [
    "# Pruning Model  Activation Mean Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ea7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nni.compression.pytorch.pruning import ADMMPruner\n",
    "from nni.compression.pytorch.pruning import ActivationMeanRankPruner\n",
    "from nni.compression.pytorch.speedup import ModelSpeedup\n",
    "import nni\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pruner_function(config_list):\n",
    "\n",
    "    model = torch.load(\"mnist_cnn.pt\")\n",
    "    model.eval()\n",
    "\n",
    "    traced_optimizer = nni.trace(optim.Adadelta)(model.parameters(), lr=1.0)\n",
    "    criterion = F.nll_loss\n",
    "    \n",
    "    # Using ADMMPruner to prune the model and generate the masks.\n",
    "    #pruner = ADMMPruner(model, config_list, trainer, traced_optimizer, criterion, iterations=5, training_epochs=1, granularity='coarse-grained')\n",
    "    \n",
    "    pruner = ActivationMeanRankPruner(model, config_list, trainer, traced_optimizer, criterion, training_batches=20)\n",
    "    \n",
    "    # show the wrapped model structure, `PrunerModuleWrapper` have wrapped the layers that configured in the config_list.\n",
    "    #print(model)\n",
    "\n",
    "    # compress the model and generate the masks\n",
    "    _, masks = pruner.compress()\n",
    "\n",
    "    # show the masks sparsity\n",
    "    print(\"Showing the masks sparsity\")\n",
    "    for name, mask in masks.items():\n",
    "        print(name, ' sparsity : ', '{:.2}'.format(mask['weight'].sum() / mask['weight'].numel()))\n",
    "\n",
    "\n",
    "    # need to unwrap the model, if the model is wrapped before speedup\n",
    "    pruner._unwrap_model()\n",
    "\n",
    "    # speedup the model, for more information about speedup, please refer :doc:`pruning_speedup`.\n",
    "    ModelSpeedup(model, torch.rand(3, 1, 28, 28).to(device), masks).speedup_model()\n",
    "\n",
    "    #print(\"Model after speedup\")\n",
    "    #print(model)\n",
    "\n",
    "    optimizer, scheduler = optimizer_scheduler_generator(model)\n",
    "    \n",
    "    # fine- tuning model compacted model\n",
    "    # tuning and evaluate the model on MNIST dataset\n",
    "    total_epoch = 3\n",
    "    \n",
    "    for epoch in range(1, total_epoch + 1):\n",
    "        train(model, device, optimizer=optimizer, epoch=epoch)\n",
    "        test(model, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53bb83e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perfomance_function(model):\n",
    "    print(\"Model after speedup\")\n",
    "    print(model)\n",
    "    \n",
    "    start = time.time()\n",
    "    best_acc = test(model, device)\n",
    "    test_time = time.time() - start\n",
    "\n",
    "    flops, params, _ = count_flops_params(model, torch.randn([3, 1, 28, 28]).to(device))\n",
    "\n",
    "    print(f'Pretrained model FLOPs {pre_flops/1e6:.2f} M, #Params: {pre_params/1e6:.2f}M, Accuracy: {pre_best_acc: .2f}%, , Test-time: {pre_test_time: .4f}s')\n",
    "    print(f'Finetuned model FLOPs {flops/1e6:.2f} M, #Params: {params/1e6:.2f}M, Accuracy: {best_acc: .2f}%, Test-time: {test_time: .4f}s, Speed-up: {pre_test_time/test_time: .2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029e382",
   "metadata": {},
   "source": [
    "## ADMM Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffcbb87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-10-03 20:16:40] \u001b[33mWARNING: The old API trainer,traced_optimizer,criterion,training_batches,activation,mode,dummy_input will be deprecated after NNI v3.0, please using the new one evaluator,training_steps,activation,mode,dummy_input\u001b[0m\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.001160\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.349021\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.076578\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.009604\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.008620\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.225321\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.177449\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.000831\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.089915\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.030283\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.061408\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.011798\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.066102\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.012305\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.029739\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.033886\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.017012\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.022536\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.042179\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.007826\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.027218\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.060657\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.003999\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.026499\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.041130\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.041660\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.109072\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.044741\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.003193\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.100599\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.055917\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.094290\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.160891\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.031844\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.010254\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.007569\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.028529\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.034850\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.080101\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.004377\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.022049\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.008405\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.040308\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.014117\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.067286\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.000140\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.123184\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.091352\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.106938\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.033343\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.087816\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.072993\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.119503\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.099822\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.040204\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.089740\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.049437\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.125966\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.107756\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.129533\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.057695\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.009546\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.019946\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.036653\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.002163\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.011559\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.062906\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.008338\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.055975\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.098696\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.004456\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.072192\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.174546\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.054976\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.202325\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.086574\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.022552\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.078206\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.048317\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.070826\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.065290\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.024872\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.164984\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.031413\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.311915\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.228545\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.067642\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.033568\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.015768\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.122570\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.027399\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.106796\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.001197\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.002323\n",
      "Showing the masks sparsity\n",
      "conv1  sparsity :  0.5\n",
      "conv2  sparsity :  0.5\n",
      "fc1  sparsity :  0.5\n",
      "[2022-10-03 20:16:56] \u001b[32mstart to speedup the model\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32minfer module masks...\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for conv1\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for .aten::relu.6\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for conv2\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for .aten::relu.7\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for .aten::max_pool2d.8\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for dropout1\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for .aten::flatten.9\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for fc1\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for .aten::relu.10\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for dropout2\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for fc2\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate mask for .aten::log_softmax.11\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the .aten::log_softmax.11\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the fc2\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the dropout2\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the .aten::relu.10\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the fc1\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the .aten::flatten.9\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the dropout1\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the .aten::max_pool2d.8\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the .aten::relu.7\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the conv2\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the .aten::relu.6\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mUpdate the indirect sparsity for the conv1\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mresolve the mask conflict\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mreplace compressed modules...\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mreplace module (name: conv1, op_type: Conv2d)\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mreplace module (name: conv2, op_type: Conv2d)\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mreplace module (name: dropout1, op_type: Dropout)\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mreplace module (name: fc1, op_type: Linear)\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mreplace linear with new in_features: 4608, out_features: 64\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mreplace module (name: dropout2, op_type: Dropout)\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mreplace module (name: fc2, op_type: Linear)\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mreplace linear with new in_features: 64, out_features: 10\u001b[0m\n",
      "[2022-10-03 20:16:56] \u001b[32mspeedup done\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakaria/.local/lib/python3.10/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.774406\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.139781\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.145159\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.206474\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.237173\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.049166\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.112457\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.069230\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.089296\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.036387\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.060088\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.083245\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.029452\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.112076\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.199143\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.235638\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.115838\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.116069\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.124952\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.103599\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.054249\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.088216\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.065464\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.146102\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.106973\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.162634\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.016124\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.159079\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.123934\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.165603\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.034368\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.071789\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.035737\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.039731\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.128509\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.196605\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.031265\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.048366\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.105133\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.106863\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.029095\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.073560\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.061796\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.024410\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.081036\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.009596\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.170375\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.054467\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.084646\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.075582\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.042796\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.211646\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.035525\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.124983\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.032870\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.077284\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.153472\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.010305\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.048623\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.003140\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.145923\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.041417\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.084915\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.074868\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.049312\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.136398\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.161954\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.078067\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.013011\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.019124\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.102247\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.094354\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.090862\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.047003\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.016259\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.040594\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.066348\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.052752\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.067946\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.063594\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.043083\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.070091\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.056954\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.098610\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.033753\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.122529\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.132022\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.218471\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.005558\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.036066\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.047090\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.030900\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.073469\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.006277\n",
      "\n",
      "Test set: Average loss: 0.0421, Accuracy: 9863/10000 (98.63%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.033979\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.029973\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.162421\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.074939\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.180933\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.047748\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.082566\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.047470\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.016455\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.024471\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.036887\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.161722\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.121831\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.014654\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.009759\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.057149\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.056034\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.141440\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.022143\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.157044\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.246035\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.068035\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.004669\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.078199\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.085870\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.024851\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.196188\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.077617\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.090399\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.016965\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.016545\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.024957\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.006201\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.265081\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.046178\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.122444\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.024508\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.023947\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.031116\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.137813\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.098199\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.009494\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.043520\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.184797\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.025940\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.026131\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.029103\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.056873\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.021748\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.165424\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.073642\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.146180\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.140887\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.015163\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.053627\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.020018\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.185996\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.058766\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.066548\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.033821\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.092594\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.006498\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.006671\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.056863\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.378284\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.007130\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.035082\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.060714\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.017676\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.012476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.137121\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.037120\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.263866\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.248862\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.008364\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.043392\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.016718\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.129828\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.080869\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.037364\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.023279\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.190189\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.047614\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.049677\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.032836\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.040715\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.014526\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.182779\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.014776\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.200945\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.032119\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.245812\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.024125\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.172638\n",
      "\n",
      "Test set: Average loss: 0.0368, Accuracy: 9882/10000 (98.82%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.015545\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.041673\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.022542\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.198784\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.021391\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.119946\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.112331\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.058464\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.011764\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.056367\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.105564\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.003424\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.033462\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.071640\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.113003\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.010428\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.052303\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.087911\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.010451\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.009611\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.246302\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.111412\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.143074\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.011399\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.011805\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.020259\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.064827\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.091435\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.066441\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.021372\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.017865\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.029068\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.123590\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.035792\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.142718\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.114591\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.004099\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.023531\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.031891\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.008086\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.176060\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.036459\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.016168\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.037715\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.009087\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.007917\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.103164\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.076704\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.017028\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.109815\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.028279\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.080836\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.089196\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.016296\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.020884\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.148924\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.153324\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.126069\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.176559\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.042736\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.017257\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.027027\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.105070\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.004102\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.015583\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.173501\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.096504\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.063451\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.256856\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.040563\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.022119\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.128419\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.007923\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.044533\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.041487\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.020941\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.031018\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.037097\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.010248\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.072458\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.243206\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.014748\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.082244\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.039148\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.059587\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.009730\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.079079\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.001939\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.072195\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.002924\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.061668\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.113091\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.137022\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.078444\n",
      "\n",
      "Test set: Average loss: 0.0359, Accuracy: 9887/10000 (98.87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_list = [{\n",
    "    'sparsity_per_layer': 0.50,\n",
    "    'op_types': ['Linear', 'Conv2d']\n",
    "}, {\n",
    "    'exclude': True,\n",
    "    'op_names': ['fc2']\n",
    "}]\n",
    "\n",
    "\n",
    "pruned_model = pruner_function(config_list=config_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5dc068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after speedup\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=4608, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Test set: Average loss: 0.0359, Accuracy: 9887/10000 (98.87%)\n",
      "\n",
      "+-------+-------+--------+----------------+-----------------+-----------------+---------+---------+\n",
      "| Index | Name  |  Type  |  Weight Shape  |    Input Size   |   Output Size   |  FLOPs  | #Params |\n",
      "+-------+-------+--------+----------------+-----------------+-----------------+---------+---------+\n",
      "|   0   | conv1 | Conv2d | (16, 1, 3, 3)  |  (3, 1, 28, 28) | (3, 16, 26, 26) |  97344  |   160   |\n",
      "|   1   | conv2 | Conv2d | (32, 16, 3, 3) | (3, 16, 26, 26) | (3, 32, 24, 24) | 2654208 |   4640  |\n",
      "|   2   | fc1   | Linear |   (64, 4608)   |    (3, 4608)    |     (3, 64)     |  294912 |  294976 |\n",
      "|   3   | fc2   | Linear |    (10, 64)    |     (3, 64)     |     (3, 10)     |   640   |   650   |\n",
      "+-------+-------+--------+----------------+-----------------+-----------------+---------+---------+\n",
      "FLOPs total: 3047104\n",
      "#Params total: 300426\n",
      "Pretrained model FLOPs 11.99 M, #Params: 1.20M, Accuracy:  99.19%, , Test-time:  1.7579s\n",
      "Finetuned model FLOPs 3.05 M, #Params: 0.30M, Accuracy:  98.87%, Test-time:  1.7093s, Speed-up:  1.03x\n"
     ]
    }
   ],
   "source": [
    "Perfomance_function(pruned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a1820",
   "metadata": {},
   "source": [
    "## ADMM Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10be2e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-10-03 20:17:43] \u001b[33mWARNING: The old API trainer,traced_optimizer,criterion,training_batches,activation,mode,dummy_input will be deprecated after NNI v3.0, please using the new one evaluator,training_steps,activation,mode,dummy_input\u001b[0m\n",
      "[2022-10-03 20:17:43] \u001b[33mWARNING: op_names ['Linear'] not found in model\u001b[0m\n",
      "[2022-10-03 20:17:43] \u001b[33mWARNING: op_names ['Linear'] not found in model\u001b[0m\n",
      "[2022-10-03 20:17:43] \u001b[33mWARNING: op_names ['Linear'] not found in model\u001b[0m\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.005200\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.178087\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.006849\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.030576\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.047069\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.389356\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.077931\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.009553\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.051433\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.041742\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.050278\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.119363\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.014803\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.002762\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.003103\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.094152\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.021208\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.001497\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.090823\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.011519\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.020035\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.003821\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.074411\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.025566\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.097333\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.056810\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.075737\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.022756\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.010429\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.081881\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.125484\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.120717\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.164688\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.164836\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.020948\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.106427\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.010338\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.164076\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.044822\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.002190\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.071733\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.004260\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.103995\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.038453\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.106123\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.092569\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.014254\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.144895\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.050726\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.000826\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.045001\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.053299\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.031417\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.036377\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.147581\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.020789\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.128446\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.007812\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.011392\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.154118\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.058524\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.028027\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.021204\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.004523\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.038255\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.021155\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.070770\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.023950\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.025620\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.050214\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.217939\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.000770\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.038088\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.035358\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.017230\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.057013\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.067089\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.052377\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.013526\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.012408\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.105138\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.127460\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.011976\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.159471\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.074681\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.038684\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.008533\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.015589\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.161677\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.094231\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.165729\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.084912\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.082565\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.002676\n",
      "Showing the masks sparsity\n",
      "conv1  sparsity :  0.5\n",
      "conv2  sparsity :  0.5\n",
      "[2022-10-03 20:17:59] \u001b[32mstart to speedup the model\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32minfer module masks...\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for conv1\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for .aten::relu.6\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for conv2\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for .aten::relu.7\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for .aten::max_pool2d.8\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for dropout1\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for .aten::flatten.9\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for fc1\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for .aten::relu.10\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for dropout2\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for fc2\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate mask for .aten::log_softmax.11\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the .aten::log_softmax.11\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the fc2\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the dropout2\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the .aten::relu.10\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the fc1\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the .aten::flatten.9\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the dropout1\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the .aten::max_pool2d.8\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the .aten::relu.7\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the conv2\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the .aten::relu.6\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mUpdate the indirect sparsity for the conv1\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mresolve the mask conflict\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mreplace compressed modules...\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mreplace module (name: conv1, op_type: Conv2d)\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mreplace module (name: conv2, op_type: Conv2d)\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mreplace module (name: dropout1, op_type: Dropout)\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mreplace module (name: fc1, op_type: Linear)\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mreplace linear with new in_features: 4608, out_features: 128\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mreplace module (name: dropout2, op_type: Dropout)\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mreplace module (name: fc2, op_type: Linear)\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mreplace linear with new in_features: 128, out_features: 10\u001b[0m\n",
      "[2022-10-03 20:17:59] \u001b[32mspeedup done\u001b[0m\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.191780\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.128892\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.061982\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.109829\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.016051\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.028111\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.048039\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.011954\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.038375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.255271\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.057768\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.013755\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.058269\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.069035\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.144401\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.049871\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.076872\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.207815\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.116002\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.113186\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.037944\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.080912\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.090280\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.122528\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.036701\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.161793\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.071117\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.001913\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.036648\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.066180\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.031281\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.040996\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.035951\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.036159\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.039109\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.023240\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.052170\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.109808\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.070747\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.076093\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.168533\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.048975\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.051575\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.177933\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.132656\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.032876\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.106274\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.065694\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.040495\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.110073\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.014750\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.034192\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.036813\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.018847\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.033507\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.024423\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.124728\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.075290\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.203790\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.086132\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.043836\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.033961\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.049951\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.024812\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.059117\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.059372\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.207459\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.025245\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.086881\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.143040\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.105235\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.054496\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.007601\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.029050\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.079818\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.028773\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.025924\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.066234\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.171117\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.058020\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.014419\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.030189\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.127526\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.112047\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.036469\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.015975\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.137257\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.030046\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.116587\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.219089\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.031239\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.088544\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.018800\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.001297\n",
      "\n",
      "Test set: Average loss: 0.0341, Accuracy: 9897/10000 (98.97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.006941\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.048711\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.071908\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.018816\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.070460\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.008164\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.021325\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.060023\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.007995\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.113383\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.028773\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.193869\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.008040\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.050209\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.004928\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.036354\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.005753\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.055461\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.065109\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.092740\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.092558\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.007290\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.029207\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.037549\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.010136\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.112467\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.040921\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.068785\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.027404\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.079750\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.125940\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.002702\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.021580\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.064160\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.014903\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.023638\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.005423\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.077633\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.071642\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.110902\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.027979\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.114877\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.083490\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.064171\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.082758\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.012564\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.015339\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.090912\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.017026\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.215999\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.086986\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.047584\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.007759\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.005329\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.029840\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.010166\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.075791\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.137323\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.033158\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.074594\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.156776\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.152478\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.017238\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.066603\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.031863\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.059361\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.011291\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.122729\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.009479\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.016867\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.029048\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.007955\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.042086\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.070098\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.003095\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.003972\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.093491\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.087200\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.043224\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.004203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.166766\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.016853\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.002031\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.012238\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.013285\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.085471\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.141067\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.048926\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.016826\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.035165\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.063727\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.024377\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.036193\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.038798\n",
      "\n",
      "Test set: Average loss: 0.0305, Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.048923\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.035140\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.017896\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.024733\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.000873\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.090227\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.029139\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.050745\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.018658\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.038381\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.007645\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.003457\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.086173\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.007330\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.054835\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.001986\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.087752\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.051961\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.000701\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.001610\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.037945\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.008225\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.052124\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.044892\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.017531\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.015703\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.032281\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.008273\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.001528\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.078580\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.102746\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.031127\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.103238\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.006220\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.000104\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.058424\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.032140\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.005032\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.096690\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.005834\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.002739\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.034736\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.011090\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.005860\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.010617\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.016423\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.008734\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.055412\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.048530\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.016871\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.100573\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.030178\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.049249\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.031123\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.033042\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.007718\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.044453\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.025775\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.000291\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.014380\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.035289\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.009595\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.020709\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.012448\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.021816\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.053311\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.006724\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.217503\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.018246\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.032278\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.129580\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.159131\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.018742\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.034708\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.037145\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.011689\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.061787\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.005378\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.025475\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.002787\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.086251\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.009143\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.002629\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.071623\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.190932\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.012297\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.035435\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.083002\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.009552\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.027742\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.142928\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.056063\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.197045\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.096234\n",
      "\n",
      "Test set: Average loss: 0.0340, Accuracy: 9887/10000 (98.87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_list = [{\n",
    "    'op_types': ['Conv2d'],\n",
    "    'total_sparsity': 0.5\n",
    "    }, {\n",
    "    'op_names': ['Linear'],\n",
    "    'total_sparsity': 0.8\n",
    "    },\n",
    "    {\n",
    "    'exclude': True,\n",
    "    'op_names': ['fc2']\n",
    "}]\n",
    "\n",
    "\n",
    "pruned_model = pruner_function(config_list=config_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a900dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after speedup\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=1008, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Test set: Average loss: 0.0498, Accuracy: 9841/10000 (98.41%)\n",
      "\n",
      "+-------+-------+--------+--------------+----------------+----------------+--------+---------+\n",
      "| Index | Name  |  Type  | Weight Shape |   Input Size   |  Output Size   | FLOPs  | #Params |\n",
      "+-------+-------+--------+--------------+----------------+----------------+--------+---------+\n",
      "|   0   | conv1 | Conv2d | (4, 1, 3, 3) | (3, 1, 28, 28) | (3, 4, 26, 26) | 24336  |    40   |\n",
      "|   1   | conv2 | Conv2d | (7, 4, 3, 3) | (3, 4, 26, 26) | (3, 7, 24, 24) | 145152 |   259   |\n",
      "|   2   | fc1   | Linear | (128, 1008)  |   (3, 1008)    |    (3, 128)    | 129024 |  129152 |\n",
      "|   3   | fc2   | Linear |  (10, 128)   |    (3, 128)    |    (3, 10)     |  1280  |   1290  |\n",
      "+-------+-------+--------+--------------+----------------+----------------+--------+---------+\n",
      "FLOPs total: 299792\n",
      "#Params total: 130741\n",
      "Pretrained model FLOPs 11.99 M, #Params: 1.20M, Accuracy:  99.19%, , Test-time:  1.7579s\n",
      "Finetuned model FLOPs 0.30 M, #Params: 0.13M, Accuracy:  98.41%, Test-time:  1.5231s, Speed-up:  1.15x\n"
     ]
    }
   ],
   "source": [
    "Perfomance_function(pruned_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
